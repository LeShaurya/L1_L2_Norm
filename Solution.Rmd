---
title: "PES University, Bangalore"
author:
- "UE21CS342AA2 - Data Analytics - Worksheet 2a - Linear and Logistic Regression"
- "Designed by Aaditya S Goel, Dept. of CSE - aadityasgoel@gmail.com"
subtitle: Established under Karnataka Act No. 16 of 2013
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
editor_options:
  markdown:
    wrap: 72
---

# Welcome to DATA Motors

India is poised to become the third largest economy in the world. To fuel and sustain this growth, Indian businesses are looking to increase their footprint and expand into different markets across the world.

DATA Motors is the leading automotive manufacturer in the country and they're now looking to enter the second largest auto market in the world, the United States of America.

But there's a catch! Pricing of cars in USA seems to be very different to that in India. Now DATA motors wants to enter this market with a bang and they need to get their pricing spot-on. So they have hired you, a consultant at the prestigious Bangalore Consulting Group. Now the onus is on you to understand what factors drive the pricing models of the most successful car companies currently in the market. Let's get to work!

## Regression

Regression is a statistical method used to model the connection between variables, understanding how changes in one influence another. It's vital for predicting outcomes, finding patterns, and making informed decisions. 

Regression is essential across diverse fields like economics and medicine due to its ability to quantify relationships and make predictions for new data. Its popularity arises from its simplicity, adaptability, and its central role in data-driven decision-making.

In this worksheet we will be exploring 3 concepts. Namely:

  - Simple Linear Regression
  
  - Multiple Linear Regression
  
  - Logistic Regression

Before we go any further, let's have a look at the dataset and it's different columns

**Data Dictionary**

    price: price of the car in dollars
    fuel_type: gas or diesel
    CompanyName: name of the manufacturer
    aspiration: std (standard or naturally aspirated engine) or turbo (turbocharged engine)
    doornumber: number of doors in the car
    carbody: type of car (sedan, wagon, hatchback, convertible, hardtop)
    drivewheel: rwd (Rear-wheel drive) or fwd(front-wheel drive)
    enginelocation: front or rear
    wheelbase: distance between front and rear axles in inches
    carlength: length of car in inches
    carwidth: width of car in inches
    carheight: height of car in inches
    curbweight: weight of car with a full tank and standard equipment
    cylindernumber: number of cylinders in the engine
    horsepower: power generated by the engine in horsepower (hp)
    mpg: fuel economy of car in miles per gallon

## Data Visualising

Let's visualize this all in the form of a Data Frame
```{r}
cars <- read.csv('Dataset_2a.csv')
head(cars,5)

```

Let us plot the distribution of car prices and see what the spread looks like.

```{r}
library(ggplot2)
library(gridExtra)

plot1 <- ggplot(cars, aes(x = price)) +
  geom_density() +
  labs(title = "Car Price Distribution Plot")

# Create the second plot (Car Price Spread)
plot2 <- ggplot(cars, aes(y = price)) +
  geom_boxplot() +
  labs(title = "Car Price Spread")

# Combine the plots and display
grid.arrange(plot1, plot2, ncol = 2)
```

We can clearly see that the prices are heavily right-skewed with some outliers. This seems to explain the exclusive, luxurious vehicles only affordable for a few.

***

## Regression Analysis

Before proceeding to a full analysis, your client DATA Motors have some questions they want you to answer.

### 1. Simple Linear Regression

From experience, they have understood that the more powerful their car is, the higher they are able to price it at to the public. They want to know if this trend holds perfectly in this new market too. Have a look at the data, pick the right variables and find the if this relationship is true. Create a scatter plot between the dependent and independent variable with the best-fit line passing through. (Hint: use the ggplot library)

MY APPROACH: PLOTTING THE GRAPHS BETWEEN THE INDEPENDANT VARIABLES THAT CAN ACTUALLY AFFECT THE PRICE OF THE CAR WRT ENGINE PERFORMANCE, SUCH AS: ASPIRATION TYPE, NO. OF CYLINDERS, TYPE OF FUEL, HORSEPOWER.



```{r}
ggplot(cars, aes(x = aspiration, y = price)) +
  geom_jitter() +
  labs(x = "Aspiration", y = "Price") +
  ggtitle("Jittered Scatterplot of Price vs. Aspiration")
```

```{r}
ggplot(cars, aes(x = cylindernumber, y = price)) +
  geom_jitter() +
  labs(x = "cylindernumber", y = "Price") +
  ggtitle("Jittered Scatterplot of Price vs. cylindernumber")
```

```{r}
ggplot(cars, aes(x = fueltype, y = price)) +
  geom_jitter() +
  labs(x = "fueltype", y = "Price") +
  ggtitle("Jittered Scatterplot of Price vs. fueltype")
```


```{r}
library(ggplot2)
df <- read.csv("Dataset_2a.csv")

scatter_plot <- ggplot(df, aes(x = horsepower, y = price)) +
  geom_point() +  
  geom_smooth(method = "lm", se = TRUE, color = "blue") +  
  labs(x = "Horsepower", y = "Price") +  # Label axes
  ggtitle("Scatter Plot of Horsepower vs. Price")

print(scatter_plot)

```

What do you infer from your graph? The results don't seem to be very surprising. But there's something that's off about the scatter plot itself. Try plotting the residuals and analyzing if it's only white noise.


FROM THE ABOVE I CAN INFER THE FOLLOWING:
1. THE PLOT BETWEEN PRICE AND ASPIRATION SHOWS THAT MANY CUSTOMERS PREFER STD VARIANT WHICH IS PRICED LOW UNLIKE THE SCATTERED VALUES OF THE TURBO VARIANT. SEEING THE EXTREME VALUES, SOME STD CARS ARE ALSO PRICED HIGHER THAN TURBO.

2. THE PLOT BETWEEN PRICE AND NO. OF CYLINDERS SHOWS THAT MAJORITY OF SALES BELONG TO 4 CYLINDER VARIANTS AND AS CYLINDERS INCREASE PEICE TENDS TO INCREASE.

3. THE PLOT BETWEEN PRICE AND FUEL TYPE SHOWS THAT MAJORITY OF SALES BELONG TO GAS VARIANTS AND SOME GAS MODELS ARE PRICED HIGHER THAN DIESEL.

4. FROM THE SCATTER PLOT BETWEEN PRICE AND HORSEPOWER, WE SEE THAT THERE IS A MILD LINEAR RELATIONSHIP, AND MANY POINTS ARE CLUSTERED AROUND THE REGRESSION LINE.



MY APPROACH -> USE THE PRICE VS HORSEPOWER INFO, AS IT IS A NUMERICAL VARIABLE, NEXT FIND THE RESIDUALS BY SUBTRACTING THE PRICES GIVEN AND THE PRICES PREDICTED ONLY USING THE HORSE POWER COLUMN & PLOT A RESIDUAL PLOT:

```{r}

## Write your code here

predicted_prices <- predict(lm(price ~ horsepower, data = df))
residuals <- df$price - predicted_prices

sd_residuals <- sd(residuals)
standardized_residuals <- residuals / sd_residuals

sd_predicted_prices <- sd(predicted_prices)
standardized_predicted_values <- predicted_prices / sd_predicted_prices

residual_plot <- ggplot(data = data.frame(standardized_residuals, standardized_predicted_values), aes(x = standardized_predicted_values, y = standardized_residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Standardized Predicted Values", y = "Standardized Residuals") +
  ggtitle("Standardized Residual Plot")

print(residual_plot)
```

How will you tackle this problem? (Hint: Think about the different kind of transformations you've learnt in class)

FROM THE ABOVE STANDARDISED RESIDUAL PLOT, WE CAN OBSERVE THAT IT IS NOT A FULLY SCATTERED DISTRIBUTION, 
INDICATING THAT THE RESIDUALS MIGHT DEPEND TO A CERTAIN EXTENT ON THE INDEPENDANT VARIABLE - HORSE POWER.


SOME OF THE TRANSFORMATION THAT WE HAVE LEARNT FROM RESIDUAL ANALYSIS ARE:
1. LOG TRANSFORMATION (OR ANY OTHER TRANSORMATIONS)
2. ADDING NEW VARIABLES.

GOING WITH APPROACH 1:


```{r}

## Write you code here


model_1 <- lm(log(price) ~ horsepower, data = df)

residuals_1 <- residuals(model_1)
sd_residuals_1 <- sd(residuals_1)

standardized_residuals_1 <- residuals_1 / sd_residuals_1
standardized_predicted_values_1 <- predict(model_1) / sd_residuals_1

residual_plot_1 <- ggplot(data = data.frame(standardized_residuals_1, standardized_predicted_values_1), aes(x = standardized_predicted_values_1, y = standardized_residuals_1)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Standardized Predicted Values (Log-Transformed)", y = "Standardized Residuals") +
  ggtitle("Standardized Residual Plot (Log-Transformed Model)")

# Print the residual plot
print(residual_plot_1)


```

AFTER TRYING OUT DIFFERENT FUNCTIONAL FORMS, THE SIN FORM HAS RETURNED THE MOST SCATTERED VALUE:

```{r}

model_2 <- lm(sin(price) ~ horsepower, data = df)

residuals_2 <- residuals(model_2)
sd_residuals_2 <- sd(residuals_2)

standardized_residuals_2 <- residuals_2 / sd_residuals_2
standardized_predicted_values_2 <- predict(model_2) / sd_residuals_2

residual_plot_2 <- ggplot(data = data.frame(standardized_residuals_2, standardized_predicted_values_1), aes(x = standardized_predicted_values_2, y = standardized_residuals_2)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Standardized Predicted Values (Sin-Transformed)", y = "Standardized Residuals") +
  ggtitle("Standardized Residual Plot (Sin-Transformed Model)")

# Print the residual plot
print(residual_plot_2)
```



***

### 2. Logistic Regression

Logistic regression is an algorithm that estimates the parameters, or coefficients, of the linear combination
of the logit model. The logistic or logit model is used to predict the probability 'p' of a binary dependent variable taking on one of two possible outcomes. This feature makes Logistic Regression useful even in problems of binary classification

DATA motors currently only build vehicles with rear-wheel drive. In America however, front-wheel drive is known to be quite popular too. Development of this technology will require significant investments into Research & Development. The client wants to know if they can recover costs quickly by charging a premium on front-wheel drive vehicles. 

Analyze the price at which these two types of cars are sold and try to find out if Front-wheel Drive cars are indeed the premium variety in the market, or if rear-wheel drive vehicles can fetch high rates. 

```{r}

## Write your code here

carsdf <- read.csv('Dataset_2a.csv')

library(ggplot2)
library(gridExtra)
ggplot(carsdf, aes(x = drivewheel, y = price,)) +
  geom_point(aes(color = factor(drivewheel)))




```

Q : Is this good news or bad news for the client? As with most things, it's a bit of both. Go ahead and think about why that might be the case here.

A : My Thoughts: The RWD cars are generally priced at a higher price range than the FWD variants. But the other side of the coin is that the number of FWD cars sold seem to be more than the number of RWD cars sold.

Meanwhile let us try and see how good our logistic regression models are performing on the data. (Hint: Use the inbuilt functions in the pROC library)

MY APPROACH -> BUILD A LOGISTIC REGRESSION MODEL AND PLOT IT, TO PREDICT THE CATEGORY OF THE DRIVE WHEEL GIVEN THE PRICE OF A CAR.(WE CAN SET A THRESHOLD AND PREDICT VALUES)

```{r}
library(dplyr)
library(ggplot2)
carsdf <- carsdf %>%
  mutate(drivewheel_encoded_1 = ifelse(drivewheel == "fwd", 0, 1) )

model_lr_1 <- glm(drivewheel_encoded_1 ~ price, data = carsdf, family = "binomial")


ggplot(carsdf, aes(x = price, y = drivewheel_encoded_1)) +
  geom_point(aes(color = factor(drivewheel_encoded_1)), pch = 19) +
  labs(x = "Price", y = "Drivewheel (0 = FWD, 1 = RWD)") +
  scale_color_manual(values = c("red", "blue")) +
  theme_minimal() +

  
  geom_smooth(method = "glm", method.args = list(family = "binomial"), 
              formula = y ~ x, color = "green", size = 1) +

  
  theme(legend.position = "none") +
  ggtitle("Logistic Regression Curve for Drivewheel(binary) vs. Price")

```
TO MONITOR HOW WELL THE LOGISTICS REGRESSION MODEL PERFORMS, WE CAN PLOT THE ROC STATISTIC AND MEASURE THE AREA UNDER THE CURVE, MORE AREA, BETTER MODEL PERFORMANCE.


```{r}
library(pROC)

roc_curve <- roc(carsdf$drivewheel_encoded_1, fitted(model_lr_1))
plot(roc_curve, col = "blue",print.auc = TRUE,plot = TRUE)
```


Q : Those are striking numbers. What does it say about our the drivewheel variable that our Logistic Regression models are able to achieve such high scores across metrics? 
A : HERE AUC SCORE OF 0.979 IS PRETTY GOOD! THUS THIS IS A VERY GOOD MODEL! THIS SAYS THAT 
1. OUR MODEL IS ABLE TO DISCRIMINATE BETWEEN FWD AND RWD VERY WELL!
2. PRICE IS AN IMPORTANT FACTOR IN PREDICITNG WETHER ITS A REAL OR FRONT WHEEL DRIVE. AND GENERALLY RWD ARE MORE EXPENSIVE.

***

### 3. Multiple Linear Regression

For our Multiple Linear Regression models, we could use all the attributes and try to predict the price. But the aim is to always predict the maximum variation in the target, with the minimum variables.

Thus, it's important to identify which features are most important to predict our target variable. Use the help of a correlogram to visually analyze the correlation between different independent variables and the one dependent variable. (Don't forget to keep an eye on the correlation between independent variables. Try and identify why it is important to do this.)

MY APPROACH -> I HAVE CONVERTED ALL THE CATEGORICAL VARIABLES TO NUMERICAL VARIABLES AND ADDED THEM AS NEW COLUMNS, AND THEN DROPPED THE CATEGORICAL COLUMNS.
NEXT, I TRY MLR WITH DIFFERENT VARIABLES BASED ON THEIR SIGNIFICANCE, ESTIMATES AND P VALUE.


```{r}
head(df,5)
```



```{r}
## Before constructing a correlogram, you will have to convert all categorical values to numerical values
## To achieve this, you can use the mutate function in the dpylr library

## Then proceed to create the correlogram
library(dplyr)

df <- df %>%
  mutate(fuel_encoded = ifelse(fueltype == "gas", 0, 1))
df <- df %>%
  mutate(aspiration_encoded = ifelse(aspiration == "std", 0, 1) )

df <- df %>%
  mutate(doornumber_encoded = ifelse(doornumber == "two", 0, 1) )

df <- df %>%
  mutate(drivewheel_encoded = ifelse(drivewheel == "fwd", 0, 1) )

df <- df %>%
  mutate(enginelocation_encoded = ifelse(enginelocation == "front", 0, 1) )

df <- df %>%
  mutate(carbody_encoded = case_when(
    carbody == "convertible" ~ 0,
    carbody == "hatchback"   ~ 1,
    carbody == "sedan"       ~ 2,
    carbody == "wagon"       ~ 3,
    carbody == "hardtop"     ~ 4
  ))


df <- df %>%
  mutate(cylindernumber_encoded = case_when(
    cylindernumber == "two" ~ 0,
    cylindernumber == "three" ~ 1,
    cylindernumber == "four" ~ 2,
    cylindernumber == "five" ~ 3,
    cylindernumber == "six" ~ 4,
    cylindernumber == "eight" ~ 5,
    cylindernumber == "twelve" ~ 6,
  ))


df <- df %>%
  select(-carbody)


df <- df %>%
  select(-fueltype)


df <- df %>%
  select(-aspiration)


df <- df %>%
  select(-doornumber)


df <- df %>%
  select(-drivewheel)


df <- df %>%
  select(-enginelocation)


df <- df %>% 
  select(-cylindernumber)


df <- df %>%
  mutate(companyname_encoded = as.integer(factor(CompanyName)))

df <- df %>% 
  select(-CompanyName)
head(df,5)

```


```{r}
#install.packages('corrplot')
library(corrplot)

cor_matrix = cor(df)

corrplot(cor_matrix, method = "color", type = "full", tl.cex = 0.9)

```



We can now see that there are features positively correlated to price, and features negatively correlated to price.
Let us use all the significant variables we have noticed in the correlogram in our Multiple Linear regression model.

Use different variables to create the Multiple Linear Regression model and analyze the difference in residual values and 
F-statistic scores between each of them.

```{r}

## Write your code here

# Create a multiple linear regression model
model_mlr <- lm(price ~ wheelbase + carlength + carwidth + curbweight + horsepower + mpg + drivewheel_encoded + cylindernumber_encoded, data = df)

summary(model_mlr)
```


```{r}
model_mlr_1 <- lm(price ~  curbweight + horsepower + mpg + cylindernumber_encoded, data = df)
summary(model_mlr_1)
```
# FINAL MODEL
```{r}
model_mlr_2 <- lm(price ~  curbweight + horsepower + carwidth + drivewheel_encoded + cylindernumber_encoded, data = df)
summary(model_mlr_2)
```


Q : What can you infer about the fit of Multiple Linear Regression on to the given dataset? 
A : Observing the Multiple R-squared value of 0.8396 and Adjusted R-squared value of 0.8356, indicate a good fit.The F-statistic has a very low p-value (< 2.2e-16), indicating that the model is statistically significant. The Residual Standard Error is could be lower, but 3329 is relatively low. Overall indicating a good fit for the data.

Q : Which are the most important variables to predict the price of the car?
A : We determine the important variables based on the value of the parameters, which can be found in hte estimate column, higher the estimate, more it contributes to the price. So 
the most important variables to predict the PRICE are:
1. cylindernumber
2. drivewheel
3. horsepower
    
because of high estimate values.

Q : How many variables did you use in your best fitting model? Which ones were they?
A : I have used 5 variables, they are:
    curbweight
    horsepower
    carwidth
    drivewheel_encoded
    cylindernumber_encoded

***

Good job with the analysis! DATA motors and Bangalore Consulting Group have both picked up valuable information from the work you just did.

***

The methods used in this worksheet form the fundamental basis for many more complex techniques and algorithms. As internship season is upon is, those of you who get to work in Data Science, Analytics etc will find yourselves using these very same techniques to answer the business questions posed by your organizations.

In a world where ChatGPT and DALL-E get all the Spotlight, classic ML techniques like Linear Regression still form the backbone of real world Analytics. The simplicity and interpretability of these models have made these models invaluable in providing insights to business owners across industries make informed, data-driven decisions.

Happy Learning!
